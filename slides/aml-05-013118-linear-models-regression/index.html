<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }

      .remark-slide-content p, .remark-slide-content li {
        font-size:40px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }

      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }

      .reset-column {
          overflow: auto;
          width: 100%;
      }

      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      } 

      .padding-top {
          padding-top: 100px;
      } 

      .remark-slide-content .smaller p, .remark-slide-content .smaller li, .remark-slide-content .smaller .remark-code{
          font-size: 25px;
      }

      .remark-slide-content .smallest p, .remark-slide-content .smallest li, .remark-slide-content .smallest .remark-code{
          font-size: 18px;
      }

      .normal {
          font-size: 30px;
      }

      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }

      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }

      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }

      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

### W4995 Applied Machine Learning

# Linear models for Regression

01/31/18

Andreas C. MÃ¼ller

???

---
class:center

# Linear Models for Regression

![:scale 50%](images/linear_regression_1d.png)

$$\hat{y} = w^T \mathbf{x} + b = \sum_{i=1}^p w_i x_i +b$$

???

---
# Ordinary Least Squares

$$\hat{y} = w^T \mathbf{x} + b = \sum_{i=1}^p w_i x_i +b $$

`$$\min_{w \in \mathbb{R}^p} \sum_{i=1}^p ||w^T\mathbf{x}_i - y_i||^2$$`

Unique solution if $\mathbf{X} = (\mathbf{x}_1, ... \mathbf{x}_n)^T$ has full rank.
???

---
# Ridge Regression

`$$ \min_{w \in \mathbb{R}^p} \sum_{i=1}^n ||w^Tx_i - y_i||^2 + \alpha ||w||^2 $$`

???
Always has a unique solution.
Has tuning parameter alpha

---
# (regularized) Empirical Risk Minimization

`$$ min_{f \in F} \sum_{i=1}^n L(f(\mathbf{x}_i), y_i) + \alpha R(f) $$`

???
FIXME pointers data fitting / regularization!
---
# Reminder on model complexity

![:scale 80%](images/overfitting_underfitting_cartoon_full.png)

---
# Boston Housing Dataset

![:scale 70%](images/boston_housing_scatter.png)
.smaller[
```python
print(X.shape)
print(y.shape)
```
```
(506, 13)
(506,)
```
]
???

---
class: middle, spacious
```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=42)

np.mean(cross_val_score(LinearRegression(), X_train, y_train, cv=10))
```
```
0.651
```


```python
np.mean(cross_val_score(Ridge(), X_train, y_train, cv=10))
```
```
0.649
```
???

---
# Coefficient of determination R^2

`$$ R^2(y, \hat{y}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \hat{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2} $$`

`$$ \bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i$$`

Can be negative for biased estimators - or the test set!

???

---
.smallest[
```python
from sklearn.model_selection import GridSearchCV
param_grid = {'alpha': np.logspace(-3, 3, 13)}
print(param_grid)
```
```
{'alpha': array([ 0.001,  0.003, 0.01, 0.032, 0.1, 0.316, 1., 3.162,
                    10., 31.623, 100., 316.228, 1000.])}
```

```python
grid = GridSearchCV(Ridge(), param_grid, cv=10)
grid.fit(X_train, y_train)
```
]
.center[
![:scale 60%](images/ridge_alpha_search.png)
]

???

---
# Adding features

```python
from sklearn.preprocessing import PolynomialFeatures, scale
poly = PolynomialFeatures(include_bias=False)
X_poly = poly.fit_transform(scale(X))
print(X_poly.shape)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y)
```
```
(506, 104)
```
```python
np.mean(cross_val_score(LinearRegression(), X_train, y_train, cv=10))
```
```
0.69
```
```python
np.mean(cross_val_score(Ridge(), X_train, y_train, cv=10))
```
```
0.81
```

???

---
.center[
![:scale 60%](images/ridge_alpha_search_poly.png)
]
.smaller[
```python
print(grid.best_params_)
print(grid.best_score_)
```
```
{'alpha': 10.0}
0.86
```
]
???

---
# Plotting coefficient values (LR)

```python
lr = LinearRegression().fit(X_train, y_train)
plt.scatter(range(X_poly.shape[1]),
            lr.coef_, c=np.sign(lr.coef_), cmap="bwr_r")
```
.center[
![:scale 55%](images/lr_coefficients_large.png)
]

???

---
# Ridge Coefficients

```python
ridge = grid.best_estimator_
plt.scatter(range(X_poly.shape[1]), ridge.coef_,
            c=np.sign(ridge.coef_), cmap="bwr_r")
```
.center[
![:scale 60%](images/ridge_coefficients.png)
]

???

---
```python
ridge100 = Ridge(alpha=100).fit(X_train, y_train)
ridge1 = Ridge(alpha=1).fit(X_train, y_train)
plt.figure(figsize=(8, 4))

plt.plot(ridge1.coef_, 'o', label="alpha=1")
plt.plot(ridge.coef_, 'o', label="alpha=14")
plt.plot(ridge100.coef_, 'o', label="alpha=100")
plt.legend()
```

.center[
![:scale 60%](images/ridge_coefficients_alpha.png)
]

???

---
# Coefficient Paths

![:scale 90%](images/ridge_coefficient_paths.png)

???

---
# Learning Curves

![:scale 90%](images/ridge_learning_curve.png)

???

---
# Lasso Regression

`$$ \min_{w \in \mathbb{R}^p} \sum_{i=1}^n ||w^T\mathbf{x}_i - y_i||^2 + \alpha ||w||_1 $$`

- Shrinks w towards zero like Ridge

- Sets some w exactly to zero - automatic feature selection!

???

---

# Understanding L1 and L2 Penalties
.wide-left-column[
![:scale 100%](images/l2_l1_l0.png)
]

.narrow-right-column[
.smaller[
`$$ \ell_2(w) = \sum_i \sqrt{w_i ^ 2}$$`
`$$ \ell_1(w) = \sum_i |w_i|$$`
`$$ \ell_0(w) = \sum_i 1_{w_i != 0}$$`
]]
???

---

# Understanding L1 and L2 Penalties

.wide-left-column[
![:scale 100%](images/l1_kink.png)
]
.narrow-right-column[
.padding-top[
.smaller[
`$ f(x) = (2 x - 1)^2 $`

`$ f(x) + L2 = (2 x - 1)^2 + \alpha x^2$`

`$ f(x) + L1= (2 x - 1)^2 + \alpha |x|$`
]]]
???

---
class: center
# Understanding L1 and L2 Penalties

![:scale 70%](images/l1l2ball.png)
???

---
class: center
# Understanding L1 and L2 Penalties

![:scale 70%](images/l1l2ball_intersect.png)
???

---
# Grid-Search for Lasso
```python
param_grid = {'alpha': np.logspace(-3, 0, 13)}
print(param_grid)
```
```
{'alpha': array([ 0.001,  0.003, 0.01, 0.032, 0.1, 0.316, 1., 3.162,
                    10., 31.623, 100., 316.228, 1000.])}
```
```python
grid = GridSearchCV(Lasso(normalize=True), param_grid, cv=10)
grid.fit(X_train, y_train)

print(grid.best_params_)
print(grid.best_score_)
```
```
{'alpha': 0.0018}
0.86
```


???

---

![:scale 90%](images/lasso_alpha_search.png)

???

---
.center[
![:scale 60%](images/lasso_coef.png)
]

```python
print(X_poly.shape)
np.sum(lasso.coef_ != 0)
```
```
(506, 104)
62
```

???

---
.smaller[
```python
from sklearn.linear_model import lars_path
# lars_path computes the exact regularization path which is piecewise linear.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
alphas, active, coefs = lars_path(X_train, y_train, eps=0.00001, method="lasso")

plt.plot(alphas, coefs.T, alpha=.5)
plt.xscale("log")
```
]
.center[
![:scale 55%](images/lars_path.png)
]
???

---
# Elastic Net

- Combines benefits of Ridge and Lasso

- two parameters to tune.


`$$\min_{w \in \mathbb{R}^p} \sum_{i=1}^n ||w^T\mathbf{x}_i - y_i||^2 + \alpha_1 ||w||_1 +  \alpha_2 ||w||^2_2 $$`

???

---
class: center
# Comparing unit balls

![:scale 70%](images/l1l2_elasticnet.png)

???

---
# Parametrization in scikit-learn
`$$\min_{w \in \mathbb{R}^p} \sum_{i=1}^n ||w^T\mathbf{x}_i - y_i||^2 + \alpha \eta ||w||_1 +  \alpha (1 - \eta) ||w||^2_2 $$`

Where $\eta$ is the relative amount of l1 penalty (`l1_ratio` in the code).
???

---
# Robust Regression
![:scale 85%](images/robust_regression.png)
???

---
# Least Squares Fit to Outlier Data

![:scale 80%](images/outliers_least_squares.png)

???

---

# Robust Fit

![:scale 80%](images/outliers_robust_fit.png)

???

---

# Huber loss

FIXME formula

![:scale 80%](images/huber_loss.png)

???

---

# RANSAC


![:scale 80%](images/ransac_algorithm.png)

???

---
class: middle
![:scale 100%](images/ransac_algorithm2.png)

???
FIXME WTF is this slide about?
---

???

---

class: middle

# Questions ?

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);

    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style'] /* removed 'code' entry*/
    }
    });
    </script>
  </body>
</html>
