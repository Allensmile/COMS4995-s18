<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }

      .remark-slide-content p, .remark-slide-content li {
        font-size:40px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }

      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }

      .reset-column {
          overflow: auto;
          width: 100%;
      }

      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      } 

      .padding-top {
          padding-top: 100px;
      } 

      .remark-slide-content .smaller p, .remark-slide-content .smaller li, .remark-slide-content .smaller .remark-code{
          font-size: 25px;
      }

      .normal {
          font-size: 30px;
      }

      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }

      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }

      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }

      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

### W4995 Applied Machine Learning

# Introduction to Supervised Learning

01/29/18

Andreas C. Müller

---

class: center

# Supervised Learning

.larger[
$$ (x_i, y_i) \propto p(x, y) \text{ i.i.d.}$$
$$ x_i \in \mathbb{R}^p$$
$$ y_i \in \mathbb{R}$$
$$f(x_i) \approx y_i$$
$$f(x) \approx y$$
]
---
class: center

# Nearest Neighbors

![:scale 40%](images/knn_boundary_dataset.png)
$$f(x) = y_i, i = \text{argmin}_j || x_j - x||$$

---
class: center

# Nearest Neighbors

![:scale 40%](images/knn_boundary_test_points.png)
$$f(x) = y_i, i = \text{argmin}_j || x_j - x||$$

---
class: center

# Nearest Neighbors

![:scale 40%](images/knn_boundary_k1.png)

$$f(x) = y_i, i = \text{argmin}_j || x_j - x||$$

---
class: center

![:scale 60%](images/train_test_set_2d_classification.png)

---
# KNN with scikit-learn

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)
print("accuracy: {:.2f}".format(knn.score(X_test, y_test)))
```
accuracy: 0.77

---
class: center

# More neighbors

![:scale 50%](images/knn_boundary_k1.png)

---
class: center

# More neighbors

![:scale 50%](images/knn_boundary_k3.png)

---
class: center, some-space

# Influence of n_neighbors

![:scale 45%](images/knn_boundary_varying_k.png)

---
class: center, spacious

# Model complexity

![:scale 75%](images/knn_model_complexity.png)

---
class: center, spacious
# Overfitting and Underfitting

![:scale 80%](images/overfitting_underfitting_cartoon_train.png)

---
class: center, spacious
# Overfitting and Underfitting

![:scale 80%](images/overfitting_underfitting_cartoon_generalization.png)

---
class: center, spacious
# Overfitting and Underfitting

![:scale 80%](images/overfitting_underfitting_cartoon_full.png)

---
class: center, spacious
# Nearest Centroid

![:scale 40%](images/nearest_centroid_boundary.png)

---
# Nearest Centroid with scikit-learn

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)

from sklearn.neighbors import NearestCentroid
nc = NearestCentroid()
nc.fit(X_train, y_train)
print("accuracy: {:.2f}".format(nc.score(X_test, y_test)))
```

```
accuracy: 0.62
```

---
class: center
# Nearest Shrunken Centroid

![:scale 70%](images/nearest_shrunken_centroid_boundary.png)

![:scale 30%](images/nearest_shrunken_centroid_thresholding.png)

---
# Computational Properties Centroids

- fit: O(n * p)
- memory: O(n_classes * p)
- predict: O(n_classes * p)

n = n_samples
p = n_features

---
# Computational Properties Neighbors
.left-column[
## Naive
- fit: no time
- memory: O(n * p)
- predict: O(n * p)
]
.right-column[
## Kd-tree
- fit: O(p * n log n)
- memory: O(n * p)
- predict:
- O(k * log(n))
  FOR FIXED p!
]
.reset-column[
n=n_samples
p=n_features
]

---
# Parametric and non-parametric models

- Parametric model:
Number of “parameters” (degrees of freedom) independent of data.

- Non-parametric model:
Degrees of freedom increase with more data.

---
class: center, spacious
# Overfitting and Underfitting

![:scale 80%](images/overfitting_underfitting_cartoon_train.png)

---
class: center, spacious
# Overfitting and Underfitting

![:scale 80%](images/overfitting_underfitting_cartoon_generalization.png)

---
class: center, spacious
# Overfitting and Underfitting

![:scale 80%](images/overfitting_underfitting_cartoon_full.png)
---
class: center, middle

![:scale 80%](images/knn_vs_nearest_centroid.png)
---
class: center, middle

![:scale 80%](images/train_test_set_2d_classification.png)

---
# Overfitting the validation set
.smaller[
```python
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import scale

data = load_breast_cancer()
X, y = data.data, data.target
X = scale(X)

X_trainval, X_test, y_trainval, y_test = train_test_split(X, y)
X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval)

knn = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)

print("Validation: {:.3f}".format(knn.score(X_val, y_val)))
print("Test: {:.3f}".format(knn.score(X_test, y_test)))
```
```
Validation: 0.972
Test: 0.951
```
]
---
# Overfitting the validation set

```python
val = []
test = []

for i in range(1000):
    rng = np.random.RandomState(i)
    noise = rng.normal(scale=.1, size=X_train.shape)
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train + noise, y_train)
    val.append(knn.score(X_val, y_val))
    test.append(knn.score(X_test, y_test))

print("Validation: {:.3f}".format(np.max(val)))
print("Test: {:.3f}".format(test[np.argmax(val)]))
```

```
Validation: 1.000
Test: 0.958
```

---

# Threefold split
.padding-top[
![:scale 100%](images/threefold_split.png)
]

--
<br />

pro: fast, simple

con: high variance, bad use of data

---
# Implementing threefold split

.smaller[
```python
X_trainval, X_test, y_trainval, y_test = train_test_split(X, y)
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval)

val_scores = []
neighbors = np.arange(1, 15, 2)
for i in neighbors:
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    val_scores.append(knn.score(X_val, y_val))
print("best validation score: {:.3f}".format(np.max(val_scores)))
best_n_neighbors = neighbors[np.argmax(val_scores)]
print("best n_neighbors: {}".format(best_n_neighbors))

knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)
knn.fit(X_trainval, y_trainval)
print("test-set score: {:.3f}".format(knn.score(X_test, y_test)))
```

```
best validation score: 0.991
best n_neighbors: 11
test-set score: 0.951
```
]
---
# Cross-validation
.padding-top[
![:scale 100%](images/kfold_cv.png)]

--
<br \>

pro: more stable, more data

con: slower

---

class: center, some-space 
# Cross-validation + test set

![:scale 70%](images/grid_search_cross_validation.png)

---
# Implementing Grid-Search with Cross-Validation

```python
from sklearn.model_selection import cross_val_score

X_train, X_test, y_train, y_test = train_test_split(X, y)

cross_val_scores = []

for i in neighbors:
    knn = KNeighborsClassifier(n_neighbors=i)
    scores = cross_val_score(knn, X_trainval, y_trainval, cv=10)
    cross_val_scores.append(np.mean(scores))
    
print("best cross-validation score: {:.3f}".format(np.max(cross_val_scores)))
best_n_neighbors = neighbors[np.argmax(cross_val_scores)]
print("best n_neighbors: {}".format(best_n_neighbors))

knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)
knn.fit(X_train, y_train)
print("test-set score: {:.3f}".format(knn.score(X_test, y_test)))
```

```
best cross-validation score: 0.967
best n_neighbors: 9
test-set score: 0.965
```

---
![:scale 100%](images/gridsearch_workflow.png)

---
# Nested Cross-Validation

- Replace outer split by CV loop
- Doesn’t yield single model
(inner loop might have different best parameter settings)
- Takes a long time, not that useful in practice

---
class: center, middle
# Cross-Validation Strategies

---
# StratifiedKFold

![:scale 100%](images/stratified_cv.png)

Stratified:
Ensure relative class frequencies in each fold reflect relative class frequencies on the whole dataset.

---
# Defaults in scikit-learn

- Three-fold is default number of folds
- For classification cross-validation is stratified
- train_test_split has stratify option:
train_test_split(X, y, stratify=y)

- No shuffle by default!

---
# Repeated KFold and LeaveOneOut

- LeaveOneOut : KFold(n_folds=n_samples)
High variance, takes a long time

- Better: repeated KFold.
Apply KFold or StratifiedKFold multiple times with shuffled data. Reduces variance!

---
# ShuffleSplit / StratifiedShuffleSplit

![:scale 100%](images/shuffle_split_cv.png)

---
# GroupKFold

![:scale 100%](images/group_kfold.png)

---
# TimeSeriesSplit

![:scale 100%](images/time_series_cv.png)

---
# Using Cross-Validation Generators

```python
from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit
kfold = KFold(n_splits=5)
skfold = StratifiedKFold(n_splits=5, shuffle=True)
ss = ShuffleSplit(n_splits=20, train_size=.4, test_size=.3)

print("KFold:\n{}".format(
      cross_val_score(KNeighborsClassifier(), X, y, cv=kfold)))

print("StratifiedKFold:\n{}".format(
      cross_val_score(KNeighborsClassifier(), X, y, cv=skfold)))

print("ShuffleSplit:\n{}".format(
      cross_val_score(KNeighborsClassifier(), X, y, cv=ss)))
```

```
KFold:
[ 0.93  0.96  0.96  0.98  0.96]
StratifiedKFold:
[ 0.97  0.95  0.98  0.96  0.96]
ShuffleSplit:
[ 0.93  0.96  0.95  0.98  0.95  0.98  0.97  0.95  0.96  0.96  0.99  0.96
  0.96  0.96  0.98  0.96  0.95  0.95  0.96  0.96]
```

---
![:scale 100%](images/gridsearch_workflow.png)

---
# GridSearchCV

```
from sklearn.model_selection import GridSearchCV

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)


param_grid = {'n_neighbors':  np.arange(1, 15, 2)}
grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10)
grid.fit(X_train, y_train)
print("best mean cross-validation score: {:.3f}".format(grid.best_score_))
print("best parameters: {}".format(grid.best_params_))

print("test-set score: {:.3f}".format(grid.score(X_test, y_test)))
```

```
best mean cross-validation score: 0.967
best parameters: {'n_neighbors': 9}
test-set score: 0.993
```

---
# GridSearchCV Results
```python
import pandas as pd
results = pd.DataFrame(grid.cv_results_)
results.columns
```
```
Index(['mean_fit_time', 'mean_score_time', 'mean_test_score',
       'mean_train_score', 'param_n_neighbors', 'params', 'rank_test_score',
       'split0_test_score', 'split0_train_score', 'split1_test_score',
       'split1_train_score', 'split2_test_score', 'split2_train_score',
       'split3_test_score', 'split3_train_score', 'split4_test_score',
       'split4_train_score', 'split5_test_score', 'split5_train_score',
       'split6_test_score', 'split6_train_score', 'split7_test_score',
       'split7_train_score', 'split8_test_score', 'split8_train_score',
       'split9_test_score', 'split9_train_score', 'std_fit_time',
       'std_score_time', 'std_test_score', 'std_train_score'],
      dtype='object')
```

```python
results.params
```
```
0     {'n_neighbors': 1}
1     {'n_neighbors': 3}
2     {'n_neighbors': 5}
3     {'n_neighbors': 7}
4     {'n_neighbors': 9}
5    {'n_neighbors': 11}
6    {'n_neighbors': 13}
Name: params, dtype: object
```

---
![:scale 100%](images/grid_search_n_neighbors.png)
---

class: middle

# Questions ?

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);

    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
