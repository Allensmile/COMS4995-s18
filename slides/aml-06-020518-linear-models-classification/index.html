<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }
      .remark-slide-content p, .remark-slide-content li {
        font-size:30px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }
      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }
      .reset-column {
          overflow: auto;
          width: 100%;
      }
      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      }
      .padding-top {
          padding-top: 100px;
      }
      .remark-slide-content .smaller p, .remark-slide-content .smaller li,
      .remark-slide-content .smaller .remark-code, .remark-slide-content .smaller a{
          font-size: 25px;
      }
      .normal {
          font-size: 30px;
      }
      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }
      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }
      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }
      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

### W4995 Applied Machine Learning

# Linear Models for Classification

02/05/18

Andreas C. Müller

???
N/A

---
class: center, middle

# Linear models for <strong>binary</strong> classfication

???
N/A

---
class: spacious

.center[
![:scale 60%](images/img_1.png)
]

$$\hat{y} = sign(w^T \textbf{x} + b) = sign(\sum\limits_{i}w_ix_i + b)$$

???

---

# Picking a loss?

$$\hat{y} = sign(w^T \textbf{x} + b)$$

`$$\min_{w \in ℝ^{p}} \sum_{i=1}^n 1_{y_i \neq sign(w^T \textbf{x} + b)}$$`

.center[
![:scale 40%](images/img_2.png)
]

???
- Obvious Idea: Minimize number of misclassifications aka 0-1 loss.
- But: non-convex, not continuous => Relax


---

# Logistic Regression


.left-column[
$$\log\left(\frac{p(y=1|x)}{p(y=0|x)}\right) = w^T\textbf{x}$$

$$p(y|\textbf{x}) = \frac{1}{1+e^{-w^T\textbf{x}}}$$

`$$\min_{w \in ℝ^{p}} -\sum_{i=1}^n log(exp(-y_iw^T \textbf{x}_i) + 1)$$`


$$\hat{y} = sign(w^T\textbf{x} + b)$$
]
.right-column[
![:scale 100%](images/img_3.png)]


???

---


# Penalized Logistic Regression

`$$\min_{w \in ℝ^{p}}-C \sum_{i=1}^n\log(exp(-y_iw^T \textbf{x}_i) + 1) + ||w||_2^2$$`

`$$\min_{w \in ℝ^{p}}-C \sum_{i=1}^n\log(exp(-y_iw^T \textbf{x}_i) + 1) + ||w||_1$$`

- C is inverse to alpha (or alpha / n_samples)

- Both versions strongly convex, l2 version smooth (differentiable).

- All points contribute to w (dense solution to dual).

???


---


# (soft margin) linear SVM

.larger[
`$$\min_{w \in ℝ^{p}}C \sum_{i=1}^n\max(0,1-y_iw^T \textbf{x}_i) + ||w||_2^2$$`

`$$\min_{w \in ℝ^{p}}C \sum_{i=1}^n\max(0,1-y_iw^T \textbf{x}_i)+ ||w||_1$$`
]

- Both versions strongly convex, neither smooth.

- Only some points contribute (the support vectors) to w (sparse solution to dual).

???

---


# SVM or LogReg?

- Do you need probability estimates?
- if [NO] --> It doesn't matter - try either / Both
- if [YES] --> Logistic Regression
- Need compact model or believe solution is sparse? Use L1

???

---

# Effect of regularization


.center[
![:scale 90%](images/img_4.png)
]

- Small C (a lot of regularization) limits the influence of individual points!

???
N/A

---

class: center, middle

# Multiclass classification


???
N/A

---

# Reduction to Binary Classification


For 4 classes
- <strong> One vs Rest </strong> [STANDARD]
  - 1v{2,3,4}, 2v{1,3,4}, 3v{1,2,4}, 4v{1,2,3}
  - n binary classifiers - each on all data

- <strong> One vs One </strong>
  - 1v1, 1v2, 1v3, 1v4, 2v3, 2v4, 3v4
  - n * (n-1) / 2 binary classifiers - each on a fraction of the data

???

---

class: spacious

# Prediction with One Vs Rest


"Class with highest score"

$$\hat{y} = arg \max_{i \in Y} \textbf{w}_i\textbf{x}$$

Unclear why it even works, but work well.


???


---

# One vs Rest Prediction

.center[
![:scale 80%](images/img_5.png)
]


???
N/A

---

# One vs Rest Prediction

.center[
![:scale 80%](images/img_6.png)
]


???
N/A

---

class: spacious

# Prediction with One Vs One

- "Vote for highest positives"
- Classify by all classifiers.
- Count how often each class was predicted.
- Return most commonly predicted class.
- Again - just a heuristic.


???

---

class: center

# One vs One Prediction

![:scale 80%](images/img_7.png)


???
N/A

---

class: center
# One vs One Prediction


![:scale 80%](images/img_8.png)


???
N/A

---

# Multinomial Logistic Regression

Probabilistic multi-class model:

.medium[
$$p(y=i|x) = \frac{e^{-\textbf{w}_i^T\textbf{x}}}{\sum e^{-\textbf{w}_j^T\textbf{x}}}$$

`$$\min_{w \in ℝ^{p}}- \sum_{i=1}^n log(p(y=y_i|x_i))$$`

$$\hat{y} = arg \max_{i \in Y} \textbf{w}_i\textbf{x}$$
]

- Same prediction rule as OvR !

???
summation equation to be modified

---

class: some-space

# In sckit-learn

- OvO: only SVC
- OvR: default for all linear models, even LogisticRegression
- LogisticRegression(multinomial=True)
- clf.decision_function = w^Tx
- logreg.predict_proba
- SVC(probability=True) not great


???
N/A
---

# Multi-Class in Practice


OvR and multinomial LogReg produce one coef per class:

.center[
![:scale 80%](images/img_9.png)
]

SVC would produce the same shape, but with different semantics!


???
N/A
---

class: center

![:scale 80%](images/img_10.png)

![:scale 80%](images/img_11.png)

(after centering data, without intercept)


???
N/A
---

class: center, middle


# Computational Considerations (for all linear models)


???
N/A
---

class: some-space

# Solver choices

- Don’t use SVC(kernel=’linear’), use LinearSVC
- For n_features >> n_samples: Lars (or LassoLars) instead of Lasso.
- For small n_samples (<10.000?), don’t worry.
- LinearSVC, LogisticRegression: dual=False if n_samples >> n_features
- LogisticRegression(solver=”sag”) for n_samples large.
- Stochastic Gradient Descent for “n_samples really
large”


???
N/A
---

class: middle


# Efficient Cross-Validation


???
N/A
---
class: some-space

# Models with build-in cross-validation

- RidgeCV() - does GCV, approximation to LOO
- LarsCV(), LassoLarsCV(), ElasticNetCV()
- Use path-algorithms to compute the full solution path.
- LogisticRegressionCV() uses warm-starts, doesn’t support all solvers.
- All have reasonable build-in parameter grids.
- For RidgeCV you can’t pick the “cv”!


???
N/A
---

class: center

# Using EstimatorCV

.center[
![:scale 100%](images/img_12.png)
]

???
N/A
---
class: center, middle

# Stochastic Gradient Descent

( see http://leon.bottou.org/projects/sgd
and http://leon.bottou.org/papers/bottou-bousquet-2008
and http://scikit-learn.org/stable/modules/scaling_strategies.html )

???
N/A

---
class: right

# Decomposing Generalization Error

.center[
![:scale 90%](images/img_13.png)
]

Bottou et. al.

???
N/A

---
class: center

.center[
![:scale 90%](images/img_14.png)
]

???
N/A

---
# The Trade-off

.center[
![:scale 90%](images/img_math_1.png)
]
- If n_max large, we are constraint by T_max!
- Making the optimization error small doesn’t matter if it means we can’t look at all training
examples!
- Trade optimization error for estimation error
use a worse algorithm but look at more data!


???

---
# Reminder: Gradient Descent

.right-column[![:scale 1-0%](images/img_15.png)]

Want: $$\arg \min_w F(w)$$

Initialize $w_0$

$$w^{(i+1)} \leftarrow w^{(i)} - \eta_i\frac{d}{dw}F(w^{(i)})$$

Converges to local minimum

???

---

# Reminder: Gradient Descent

.center[
![:scale 40%](images/img_16.png)
]

$$w^{(i+1)} \leftarrow w^{(i)} - \eta_i\frac{d}{dw}F(w^{(i)})$$


???


---

# Pick a learning rate

.center[
![:scale 90%](images/img_17.png)
]

$$w^{(i+1)} \leftarrow w^{(i)} - \eta_i\frac{d}{dw}F(w^{(i)})$$

???

---

# Stochastic Gradient Descent

- Logistic Regression:

`$$F(w) = -C \sum_{i=1}^n\log(exp(-y_iw^T \textbf{x}_i) +1 ) + ||w||_2^2$$`


- Pick $x_i$ randomly, then

`$$\frac{d}{dw}F(w) = \frac{d}{dw} -C \log(exp(-y_iw^T \textbf{x}_i) +1 ) + \frac{1}{n}||w||_2^2$$`

- Is a stochastic approximation of gradient of F with expectation the actual gradient.
- In practice: just iterate over i.


???

---
class: spacious

# SGD and partial_fit

- SGDClassifier(), SGDRegressor() fast on very large datasets – have many different loss and regularization options.
- Tuning learning rate and schedule can be tricky. “optimal” learning rate only works with unitvariance data. Averaging can be helpful.
- partial_fit allows working with out-of-memory data!


???

---
class: center, spacious

![:scale 90%](images/img_18.png)

![:scale 90%](images/img_19.png)



???
N/A

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
