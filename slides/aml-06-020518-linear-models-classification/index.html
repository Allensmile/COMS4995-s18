<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }
      .remark-slide-content p, .remark-slide-content li {
        font-size:30px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }
      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }
      .reset-column {
          overflow: auto;
          width: 100%;
      }
      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre, .remark-slide-content .compact .MathJax_Display{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      }
      .padding-top {
          padding-top: 100px;
      }
      .remark-slide-content .smaller p, .remark-slide-content .smaller li,
      .remark-slide-content .smaller .remark-code, .remark-slide-content .smaller a{
          font-size: 25px;
      }
      .normal {
          font-size: 30px;
      }
      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }
      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }
      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }
      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

### W4995 Applied Machine Learning

# Linear Models for Classification

02/05/18

Andreas C. Müller

???
Today we're going to talk about linear models for classification, and in addition to that
some general principles and advanced topics surrounding general models, both for classification
and regression.
---
class: center, middle

# Linear models for <strong>binary</strong> classfication

???
We'll first start with linear models for binary classification, so if there are only two classes.
That makes the models much easier to understand.

---
class: spacious

.center[
![:scale 60%](images/img_1.png)
]

$$\hat{y} = \text{sign}(w^T \textbf{x} + b) = \text{sign}\left(\sum\limits_{i}w_ix_i + b\right)$$

???
Similar to the regression case, basically all linear models for classification have the same way
to make predictions. As with regression, they compute an inner product of a weight vector w with
the feature vector x, and add some bias b. The result of that is a real number, as in regression.
For classification, however, we only look at the sign of the result, so whether it is negative
or positive. If it's positive, we predict one class, usually called +1, if it's negative,
we predict the other class, usually called -1. If the result is 0, by convention
the positive class is predicted, but because it's a floating point number that doesn't really
happen in practice. You'll see that sometimes in my notation I will not have a $b$. That's because
you can always add a constant feature to x to achieve the same effect (thought you would then need
to leave that feature out of the regularization). So when I write $w^Tx$ without a $b$ assume
that there is a constant feature added that is not part of any regularization.

Geometrially, what the formula means is that the decision boundary of a linear
classifier will be a hyperplane in the feature space, where w is the normal vector of that plane.
In the 2d example here, it's just a line separating red and blue. Everything on the right hand side
would be classified as blue by this classifier, and everything on the left-hand side as red.

FIXME: add w and b to image

Questions?
So again, the learning here consists of finding parameters w and b based on the training set,
and that is where the different algorithms differ. There are quite a lot of algorithms out
there, and there are also quite a lot in scikit-learn, but we'll only discuss the most common ones.

The most straight-forward way to approach finding w and b is to use the framework
of empirical risk minimization that we talked about last time, so finding parameters that
minimize some loss o the training set. Where classification differs quite a bit from
regression is on how we want to measure misclassifications.


---

# Picking a loss?

$$\hat{y} = \text{sign}(w^T \textbf{x} + b)$$

`$$\min_{w \in ℝ^{p}} \sum_{i=1}^n 1_{y_i \neq \text{sign}(w^T \textbf{x} + b)}$$`

.center[
![:scale 40%](images/img_2.png)
]

???
So we need to define a loss function for given w and b that tell us how well they fit the training set.
Obvious Idea: Minimize number of misclassifications aka 0-1 loss
but this loss is non-convex, not continuous and minimizing it is NP-hard.
So we need to relax it, which basically means we want to find a convex upper bound for this loss.
This is not done on the actual prediction, but on the inner product $w^T x$, which is also called the decision function.
So this graph here has the inner product on the x axis, and shows what the loss would be for class 1.
The 0-1 loss is zero if the decision function is positive, and one if it's negative.
Because a positive decision function means a positive predition, means correct classification in the case of y=1.
A negative prediction means a wrong classification, which is penalized by the 0-1 loss with a loss of 1, i.e. one mistake.

The other losses we'll talk about are mostly the hinge loss and the log loss. You can see they are both upper bounds on the
0-1 loss but they are convex and continuous.
Both of these losses care not only that you make a correct prediction, but also "how correct" your prediction is, i.e.
how positive or negative your decision function is.
We'll talk a bit more about the motivation of these two losses, starting with the logistic loss.


---

# Logistic Regression


.left-column[
$$\log\left(\frac{p(y=1|x)}{p(y=0|x)}\right) = w^T\textbf{x}$$

$$p(y|\textbf{x}) = \frac{1}{1+e^{-w^T\textbf{x}}}$$

`$$\min_{w \in ℝ^{p}} -\sum_{i=1}^n \log(\exp(-y_iw^T \textbf{x}_i) + 1)$$`


$$\hat{y} = \text{sign}(w^T\textbf{x} + b)$$
]
.right-column[
![:scale 100%](images/img_3.png)]


???
Logistic regression is probably the most commonly used linear classifier, maybe the most commonly used classifier overall.
The idea is to model the log-odds, which is log p(y=1|x) - log p(y=0|x) as a linear function, as shown here.
Rearranging the formula, you get a model of p(y=1|x) as 1 over 1 + ...
This function is called the logistic sigmoid, and is drawn to the right here. Basically it squashed the linear function $w^Tx$
between 0 and 1, so that it can model a probability.

Given this equation for p(y|x), what we want to do is maximize the probability of the training set under this model.
This approach is known as maximum likelihood. Basically you want to find w and b such that they assign maximum
probability to the labels observed in the training data. You can rearrange that a bit and end up with this equation here,
which contains the log-loss as seen on the last slide.

The prediction is the class with the higher probability. In the binary case, that's the same as asking whether
the probability of class 1 is bigger or smaller than .5.
And as you can see from the plot of the logistic sigmoid, the probability of the class +1 is greater than .5
exactly if the decision function $w^T x$ is greater than 0.
So predicting the class with maximum probability is the same as predicting which side of the hyperplane
given by w we are on.

Ok so this is logistic regression. We minimize this loss and get a w which defines a hyper plane.
But if you think back to last time, this is only part of what we want. This formulation
tries to fit the training data, but it doesn't care about finding a simple solution.

---


# Penalized Logistic Regression

`$$\min_{w \in ℝ^{p}}-C \sum_{i=1}^n\log(\exp(-y_iw^T \textbf{x}_i) + 1) + ||w||_2^2$$`

`$$\min_{w \in ℝ^{p}}-C \sum_{i=1}^n\log(\exp(-y_iw^T \textbf{x}_i) + 1) + ||w||_1$$`

- C is inverse to alpha (or alpha / n_samples)

- Both versions strongly convex, l2 version smooth (differentiable).

- All points contribute to $w$ (dense solution to dual).

???

So we can do the same we did for regression: we can add regularization terms using the L1 and L2 norm.
The effects are the same as for regression: both push the coefficients towards zero, but the l1 norm
encourages coefficients to be exactly zero, for the same reasons we discussed last time.

You could also use a mixed penalty to get something like the elasticnet. That's not implemented
in the logisticregression class in scikit-learn right now, but it's certainly a sensible thing to do.

Here I used a slightly different notation as last time, though. I'm not using alpha to multiply the
regularizer, instead I'm using C to multiply the loss. That's mostly because that's how it's done
in scikit-learn and it has only historic reasons. The idea is exactly the same, only now C is 1 over alpha.
So large C means heavy weight to the loss, means little regularization, while small C means less weight on the
loss, means strong regularization.

Depending on the model, there might be a factor of n_samples in there somewhere. Usually we try to make
the objective as independent of the number of samples as possible in scikit-learn, but that might
lead to surprises if you're not aware of it.

Some side-notes on the optimization problem: here, as in regression, having more regularization
makes the optimization problem easier. You might have seen this in your homework already, if
you decrease C, meaning you add more regularization, your model fits more quickly.

One particular property of the logistic loss, compared to the hinge loss we'll discuss next is that
each data point contributes to the loss, so each data point has an effect on the solution.
That's also true for all the regression models we saw last time.

---


# (soft margin) linear SVM

.larger[
`$$\min_{w \in ℝ^{p}}C \sum_{i=1}^n\max(0,1-y_iw^T \textbf{x}_i) + ||w||_2^2$$`

`$$\min_{w \in ℝ^{p}}C \sum_{i=1}^n\max(0,1-y_iw^T \textbf{x}_i)+ ||w||_1$$`
]

- Both versions strongly convex, neither smooth.

- Only some points contribute (the support vectors) to $w$ (sparse solution to dual).

???
Moving from logistic regression to linear SVMs is just a matter of changing the loss
from the log loss to the hinge loss. The hinge-loss is defined as ...
And we can penalize using either l1 or l2 norm, or again, in principle also elastic net.
This formulation with the hinge loss doesn't really make sense without the penalty,
because of the formulation of the hinge loss.
What this loss says is basically "if you predict the right class with a margin of 1, there is no loss".
Otherwise the loss is linear in the decision function. So you need to be on the right side
of the hyperplane by a given amount, and then there is no more loss.
That's the reason you need the penalty, for the 1 to make sense. Otherwise you could just scale up
$w$ to make it far enough on the right side. But the regularization penalizes growing $w$.

The hinge loss has a kink, same as the l1 norm, and so it's not a smooth optimization problem
any more, but that's not really a big deal. What's interesting is that all the points
that are classified correctly with a margin of at least 1 have a loss of zero, and so they
don't influence the solution any more.
All the point that are not classified correctly by this margin are the ones
that do influence the solution and they are called the support vectors.


FIXME loss image again?
FIXME graph of not influencing the solution?

FIXME: motivate SVM?

---
class: center
# Logistic Regression vs SVM
.compact[
`$$\min_{w \in ℝ^{p}}-C \sum_{i=1}^n\log(\exp(-y_iw^T \textbf{x}_i) + 1) + ||w||_2^2$$`
`$$\min_{w \in ℝ^{p}}C \sum_{i=1}^n\max(0,1-y_iw^T \textbf{x}_i) + ||w||_2^2$$`

![:scale 45%](images/img_2.png)
]
???
FIXME remove squared hinge?

So this is the main difference between logistic regression and linear SVMs:
Does it penalize misclassifications according to the green line, or according to the blue line?
In practice it doesn't make a big difference.

---


# SVM or LogReg?

- Do you need probability estimates?
- if [NO] --> It doesn't matter - try either / Both
- if [YES] --> Logistic Regression
- Need compact model or believe solution is sparse? Use L1

???
So which one of them should you use? If you need probability estimates,
you should use logistic regression. If you don't, you can pick either, and it doesn't really matter.
Logistic regression can be a bit faster to optimize in theory.
If you're in a setting where there's many more feature than samples, it might make sense to use linear SVMs and solve the dual,
but you can actually solve either of the problems in the dual, and we'll talk about what that means in practice in a little bit.

---

# Effect of regularization


.center[
![:scale 90%](images/img_4.png)
]

- Small C (a lot of regularization) limits the influence of individual points!

???

So I spared you with coefficient plots, because they looks the same as for regression. All the things I said about model
complexity and dependency on the number of features and samples is as true for classification as it is for regression.

There is another interesting way to thing about regularization that I found helpful, though.
I'm not going to walk through the math for this, but you can reformulate the optimization problem and find that what the C
parameter does is actually limit the influence of individual data points. With very large C, we said we have no regularization.
It also means individual data points can have basically unlimited influence, as you can see here. There are two outliers
here, which basically completely tilt the decision boundary.
But if we decrease C, and therefore increase the regularization, what happens is that the influence of these outlier points
becomes limited, and the other points get more influence.

FIXME illustration for support vectors?

---

class: center, middle

# Multiclass classification


???

---

# Reduction to Binary Classification


For 4 classes
- <strong> One vs Rest </strong> [STANDARD]
  - 1v{2,3,4}, 2v{1,3,4}, 3v{1,2,4}, 4v{1,2,3}
  - n binary classifiers - each on all data

- <strong> One vs One </strong>
  - 1v1, 1v2, 1v3, 1v4, 2v3, 2v4, 3v4
  - n * (n-1) / 2 binary classifiers - each on a fraction of the data

???

---

class: spacious

# Prediction with One Vs Rest


"Class with highest score"

$$\hat{y} = \text{arg}\max_{i \in Y} \textbf{w}_i\textbf{x}$$

Unclear why it even works, but work well.


???


---

# One vs Rest Prediction

.center[
![:scale 80%](images/img_5.png)
]


???
N/A

---

# One vs Rest Prediction

.center[
![:scale 80%](images/img_6.png)
]


???
N/A

---

class: spacious

# Prediction with One Vs One

- "Vote for highest positives"
- Classify by all classifiers.
- Count how often each class was predicted.
- Return most commonly predicted class.
- Again - just a heuristic.


???

---

class: center

# One vs One Prediction

![:scale 80%](images/img_7.png)


???
N/A

---

class: center
# One vs One Prediction


![:scale 80%](images/img_8.png)


???
N/A

---

# Multinomial Logistic Regression

Probabilistic multi-class model:

.medium[
$$p(y=i|x) = \frac{e^{-\textbf{w}_i^T\textbf{x}}}{\sum e^{-\textbf{w}_j^T\textbf{x}}}$$

`$$\min_{w \in ℝ^{p}}- \sum_{i=1}^n \log(p(y=y_i|x_i))$$`

$$\hat{y} = \text{arg} \max_{i \in Y} \textbf{w}_i\textbf{x}$$
]

- Same prediction rule as OvR !

???
summation equation to be modified

---

class: some-space

# In sckit-learn

- OvO: only SVC
- OvR: default for all linear models, even LogisticRegression
- LogisticRegression(multinomial=True)
- clf.decision_function = w^Tx
- logreg.predict_proba
- SVC(probability=True) not great


???
N/A
---

# Multi-Class in Practice


OvR and multinomial LogReg produce one coef per class:

.center[
![:scale 80%](images/img_9.png)
]

SVC would produce the same shape, but with different semantics!


???
N/A
---

class: center

![:scale 80%](images/img_10.png)

![:scale 80%](images/img_11.png)

(after centering data, without intercept)


???
N/A
---

class: center, middle


# Computational Considerations (for all linear models)


???
N/A
---

class: some-space

# Solver choices

- Don’t use SVC(kernel=’linear’), use LinearSVC
- For n_features >> n_samples: Lars (or LassoLars) instead of Lasso.
- For small n_samples (<10.000?), don’t worry.
- LinearSVC, LogisticRegression: dual=False if n_samples >> n_features
- LogisticRegression(solver=”sag”) for n_samples large.
- Stochastic Gradient Descent for “n_samples really
large”


???
N/A
---

class: middle


# Efficient Cross-Validation


???
N/A
---
class: some-space

# Models with build-in cross-validation

- RidgeCV() - does GCV, approximation to LOO
- LarsCV(), LassoLarsCV(), ElasticNetCV()
- Use path-algorithms to compute the full solution path.
- LogisticRegressionCV() uses warm-starts, doesn’t support all solvers.
- All have reasonable build-in parameter grids.
- For RidgeCV you can’t pick the “cv”!


???
N/A
---

class: center

# Using EstimatorCV

.center[
![:scale 100%](images/img_12.png)
]

???
N/A
---
class: center, middle

# Stochastic Gradient Descent

( see http://leon.bottou.org/projects/sgd
and http://leon.bottou.org/papers/bottou-bousquet-2008
and http://scikit-learn.org/stable/modules/scaling_strategies.html )

???
N/A

---
class: right

# Decomposing Generalization Error

.center[
![:scale 90%](images/img_13.png)
]

Bottou et. al.

???
N/A

---
class: center

.center[
![:scale 90%](images/img_14.png)
]

???
N/A

---
# The Trade-off

.center[
![:scale 90%](images/img_math_1.png)
]
- If n_max large, we are constraint by T_max!
- Making the optimization error small doesn’t matter if it means we can’t look at all training
examples!
- Trade optimization error for estimation error
use a worse algorithm but look at more data!


???

---
# Reminder: Gradient Descent

.right-column[![:scale 1-0%](images/img_15.png)]

Want: $$\arg \min_w F(w)$$

Initialize $w_0$

$$w^{(i+1)} \leftarrow w^{(i)} - \eta_i\frac{d}{dw}F(w^{(i)})$$

Converges to local minimum

???

---

# Reminder: Gradient Descent

.center[
![:scale 40%](images/img_16.png)
]

$$w^{(i+1)} \leftarrow w^{(i)} - \eta_i\frac{d}{dw}F(w^{(i)})$$


???


---

# Pick a learning rate

.center[
![:scale 90%](images/img_17.png)
]

$$w^{(i+1)} \leftarrow w^{(i)} - \eta_i\frac{d}{dw}F(w^{(i)})$$

???

---

# Stochastic Gradient Descent

- Logistic Regression:

`$$F(w) = -C \sum_{i=1}^n\log(exp(-y_iw^T \textbf{x}_i) +1 ) + ||w||_2^2$$`


- Pick $x_i$ randomly, then

`$$\frac{d}{dw}F(w) = \frac{d}{dw} -C \log(exp(-y_iw^T \textbf{x}_i) +1 ) + \frac{1}{n}||w||_2^2$$`

- Is a stochastic approximation of gradient of F with expectation the actual gradient.
- In practice: just iterate over i.


???

---
class: spacious

# SGD and partial_fit

- SGDClassifier(), SGDRegressor() fast on very large datasets – have many different loss and regularization options.
- Tuning learning rate and schedule can be tricky. “optimal” learning rate only works with unitvariance data. Averaging can be helpful.
- partial_fit allows working with out-of-memory data!


???

---
class: center, spacious

![:scale 90%](images/img_18.png)

![:scale 90%](images/img_19.png)



???
N/A

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
